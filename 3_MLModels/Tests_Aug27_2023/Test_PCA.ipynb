{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c99eff4-125a-49ef-afaf-558b0df7144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 13:57:20.790586: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "from keras.utils import to_categorical\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3de435-d5d1-4c9f-a3ae-8cb02b3823d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/glade/u/home/jhayron/WR_Predictability/3_MLModels/\")\n",
    "from model_builders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e27827-1a70-49a2-a2ce-4e8c73dd3520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318738c5-e2f8-4584-97e7-09a07e6feb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c43fd-d289-4bab-b0e0-7a67ddcc3861",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da316b8f-91ad-4fc3-a637-59540ace8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale_image_time_series(image_time_series):\n",
    "    \"\"\"\n",
    "    Perform min-max scaling on a 3D image time series for each pixel's time series.\n",
    "\n",
    "    Parameters:\n",
    "        image_time_series (numpy.ndarray): A 3D NumPy array representing the image time series.\n",
    "            The shape should be (num_samples, height, width), where num_samples is the number of time steps.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 3D NumPy array with the same shape as the input, but with scaled values.\n",
    "    \"\"\"\n",
    "    # Reshape the input to (num_samples, num_pixels) for scaling\n",
    "    num_samples, height, width = image_time_series.shape\n",
    "    image_time_series_reshaped = image_time_series.reshape((num_samples, -1))\n",
    "\n",
    "    # Initialize the MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit and transform the scaler to each pixel's time series separately\n",
    "    scaled_time_series = scaler.fit_transform(image_time_series_reshaped.T).T\n",
    "\n",
    "    # Reshape the scaled data back to the original shape\n",
    "    scaled_image_time_series = scaled_time_series.reshape((num_samples, height, width))\n",
    "\n",
    "    return scaled_image_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d65e0089-54e7-4056-9265-2504ea04161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tf_datasets(input_data, output_data):\n",
    "#     # Convert xarray dataset to numpy array for TensorFlow Dataset\n",
    "#     input_images = input_data.transpose('time', 'lat', 'lon','channel').values\n",
    "#     output_one_hot = output_data.values\n",
    "\n",
    "#     # Create TensorFlow Datasets\n",
    "#     input_dataset = tf.data.Dataset.from_tensor_slices(input_images)\n",
    "#     output_dataset = tf.data.Dataset.from_tensor_slices(output_one_hot)\n",
    "\n",
    "#     # Combine input and output datasets into a joint dataset\n",
    "#     joint_dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\n",
    "\n",
    "#     return joint_dataset\n",
    "def create_tf_datasets(input_data, output_data):\n",
    "    # Convert xarray dataset to numpy array for TensorFlow Dataset\n",
    "    input_images = input_data.transpose('time', 'pc').values\n",
    "    output_one_hot = output_data.values\n",
    "\n",
    "    # Create TensorFlow Datasets\n",
    "    input_dataset = tf.data.Dataset.from_tensor_slices(input_images)\n",
    "    output_dataset = tf.data.Dataset.from_tensor_slices(output_one_hot)\n",
    "\n",
    "    # Combine input and output datasets into a joint dataset\n",
    "    joint_dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\n",
    "\n",
    "    return (input_images,output_one_hot)\n",
    "\n",
    "def create_datasets(input_anoms, var_name, df_shifts, week_out):\n",
    "# Assuming you have the xarray.Dataset 'input_data' and the pandas.Series 'output_data'\n",
    "    input_data = copy.deepcopy(input_anoms[var_name])\n",
    "\n",
    "    array_temp = input_data.data\n",
    "    array_temp[np.isfinite(array_temp)==False]=0\n",
    "    # print(array_temp[0].min(),array_temp[0].max())\n",
    "    # array_temp = min_max_scale_image_time_series(array_temp)\n",
    "    # print(array_temp[0].min(),array_temp[0].max())\n",
    "    input_data.data = array_temp\n",
    "\n",
    "#     input_data = (input_data - input_data.mean('time')) / (input_data.std('time'))\n",
    "    \n",
    "#     input_data[np.isfinite(array_temp)==False] = 0\n",
    "    \n",
    "    # Reshape the data to add a new dimension\n",
    "    values_reshaped = input_data.values.reshape(input_data.shape[0], input_data.shape[1])\n",
    "\n",
    "    # Create a new xarray.DataArray with the reshaped data and the original coordinates\n",
    "    input_data = xr.DataArray(values_reshaped, coords=input_data.coords, dims=('time', 'pc'))\n",
    "    output_data = copy.deepcopy(df_shifts[f'week{week_out}']).dropna()\n",
    "\n",
    "    # Step 1: Create a common date index that includes all dates in both the input and output data\n",
    "    common_dates = np.intersect1d(input_data['time'].values, output_data.index)\n",
    "\n",
    "    # Step 2: Reindex the input xarray dataset and the output DataFrame to the common date index\n",
    "    input_data = input_data.sel(time=common_dates)\n",
    "    output_data = output_data.loc[common_dates]\n",
    "\n",
    "    # Step 3: One-hot encode the output DataFrame using to_categorical\n",
    "    num_classes = len(output_data.unique())  # Number of classes (number of weeks in this case)\n",
    "    output_data_encoded = to_categorical(output_data, num_classes=num_classes)\n",
    "    output_data_encoded = pd.DataFrame(output_data_encoded,index=output_data.index)\n",
    "\n",
    "    # Step 4: Create masks for training, validation, and testing periods\n",
    "    train_mask = (output_data.index >= '1980-01-01') & (output_data.index <= '2010-12-31')\n",
    "    val_mask = (output_data.index >= '2011-01-01') & (output_data.index <= '2015-12-31')\n",
    "    test_mask = (output_data.index >= '2016-01-01') & (output_data.index <= '2020-12-31')\n",
    "\n",
    "    # Step 5: Split the input xarray dataset and the output DataFrame into subsets\n",
    "    input_train = input_data.sel(time=train_mask)\n",
    "    input_val = input_data.sel(time=val_mask)\n",
    "    input_test = input_data.sel(time=test_mask)\n",
    "\n",
    "    output_train = output_data_encoded.loc[train_mask]\n",
    "    output_val = output_data_encoded.loc[val_mask]\n",
    "    output_test = output_data_encoded.loc[test_mask]\n",
    "\n",
    "    train_joint_dataset = create_tf_datasets(input_train, output_train)\n",
    "    val_joint_dataset = create_tf_datasets(input_val, output_val)\n",
    "    test_joint_dataset = create_tf_datasets(input_test, output_test)\n",
    "\n",
    "    # buffer_size = train_joint_dataset.cardinality()\n",
    "    # train_joint_dataset = train_joint_dataset.shuffle(buffer_size)\n",
    "    return train_joint_dataset, val_joint_dataset, test_joint_dataset\n",
    "\n",
    "def get_output_from_dataset(dataset):\n",
    "    output_array = []\n",
    "    for input_data, output_data in dataset.as_numpy_iterator():\n",
    "        output_array.append(output_data)\n",
    "\n",
    "    # Convert the list of NumPy arrays into a single NumPy array\n",
    "    output_array = np.array(output_array)\n",
    "    return output_array\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    return tf.py_function(balanced_accuracy_score, (y_true, y_pred), tf.float32)\n",
    "\n",
    "def logging_callback(study, frozen_trial):\n",
    "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "    if previous_best_value != study.best_value:\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "        print(\n",
    "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
    "            frozen_trial.number,\n",
    "            frozen_trial.value,\n",
    "            frozen_trial.params,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989471a-eaaa-43c5-8f51-599709071288",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c479563a-1e9e-4926-b735-a3ea74f46c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 13:58:10.221400: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-28 13:58:10.222903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-28 13:58:10.256990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:8a:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-08-28 13:58:10.257033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-28 13:58:10.555183: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-28 13:58:10.555279: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-28 13:58:10.652325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-28 13:58:10.689077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-28 13:58:10.733733: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-28 13:58:10.766446: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-28 13:58:10.954285: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-28 13:58:10.955152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "## GLOBAL SEED ##    \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafd86b-e251-4679-84d1-00e6525bac7f",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b53bc968-2cd8-47e2-aac1-ce05447ce373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get variable and origin from command-line arguments\n",
    "week_out = 1\n",
    "\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "week_out_str = f'week{week_out}'\n",
    "\n",
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "70f4c017-84ed-4866-a505-3d312313367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principal_components(input_anoms, var_name, training_period):\n",
    "    # Step 1: Select the training period\n",
    "    input_anoms_flat = input_anoms.stack(flat=('lat','lon')).transpose('time','flat')[var_name]\n",
    "\n",
    "    input_anoms_flat_train = input_anoms_flat.sel(time=training_period)\n",
    "\n",
    "    # create pca object\n",
    "    pca_obj = PCA(24, whiten=True)\n",
    "    # fit pca with data\n",
    "    pca_obj = pca_obj.fit(input_anoms_flat_train)\n",
    "\n",
    "    # transform era5 data with pca\n",
    "    anoms_transformed = pca_obj.transform(input_anoms_flat)\n",
    "\n",
    "    # print(f'Variance explained: {pca_obj.explained_variance_ratio_ * 100}')\n",
    "    # print(\n",
    "    # f'Cumulative sum of variance explained for EOF1 and EOF2: {np.cumsum(pca_obj.explained_variance_ratio_) * 100}'\n",
    "    # )\n",
    "        \n",
    "    # Create a dataset and assign data arrays to it\n",
    "    transformed_dataset = xr.Dataset(\n",
    "        {\n",
    "            var_name: ([\"time\", \"pc\"], anoms_transformed),\n",
    "        },\n",
    "        coords={\"time\": input_anoms_flat.time, \"pc\": np.arange(anoms_transformed.shape[-1])},\n",
    "    )\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6276ec8f-20ec-49eb-80a9-a9d3c2e820ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normalize(data_array, training_period):\n",
    "    # Calculate the minimum and maximum values along the time dimension\n",
    "    min_vals = data_array.sel(time=training_period).min(dim=\"time\")\n",
    "    max_vals = data_array.sel(time=training_period).max(dim=\"time\")\n",
    "    \n",
    "    # Normalize the data using min-max scaling\n",
    "    normalized_data = (data_array - min_vals) / (max_vals - min_vals)\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ab78ba27-fb40-467e-83f8-349b24b18e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'Z500_ERA5'\n",
    "\n",
    "path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd/'\n",
    "\n",
    "input_anoms = xr.open_dataset(f'{path_weekly_anoms}{name_var}.nc')\n",
    "var_name = list(input_anoms.data_vars.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a78af9f4-295f-41a5-a434-8caa9f02870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_period = slice('1980-01-01', '2015-12-31')  # Define your training period\n",
    "principal_components = get_principal_components(input_anoms, var_name, training_period)\n",
    "# Apply the min-max normalization to each pc data variable\n",
    "pc_normalized = copy.deepcopy(principal_components)\n",
    "arr_pc = pc_normalized[var_name].values\n",
    "\n",
    "for pc in principal_components.pc.values:\n",
    "    arr_pc[:,pc] = minmax_normalize(principal_components[var_name].sel(pc=pc),\n",
    "                                   training_period\n",
    "                                   )\n",
    "pc_normalized[var_name].data = arr_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ad2b2fe0-8774-425e-8407-d6c74df544de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_joint_dataset, val_joint_dataset, test_joint_dataset = \\\n",
    "    create_datasets(pc_normalized, var_name, df_shifts, week_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ec24f680-ffce-42dd-b461-a813ed1d97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_random = np.arange(len(train_joint_dataset[0]))\n",
    "np.random.shuffle(index_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bb8bb022-2c79-477a-ad51-724fb7a8a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, AveragePooling2D, Dropout, BatchNormalization,SpatialDropout2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "123cb03e-399b-4358-99de-cfae19e1c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define baseline model. Then use it in Keras Classifier for the training\n",
    "def baseline_model():\n",
    "    # Create model here\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim = 24, activation = ReLU())) # Rectified Linear Unit Activation Function\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(125, activation = ReLU()))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(25, activation = ReLU()))\n",
    "\n",
    "    model.add(Dense(5, activation = 'softmax')) # Softmax for multi-class classification\n",
    "    # Compile model here\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  optimizer = keras.optimizers.Adam(lr=0.001), \n",
    "                  metrics=[balanced_accuracy,'accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1fb295ca-ed4a-42d3-bd16-00a32ef27239",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '/glade/work/jhayron/Data4Predictability/models/CNN_Aug28_2023/pca/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "333f8d33-625c-4a45-80de-76a10460e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "32070624-5b5a-4746-8ebc-44b95c2d4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'weighted_loss':True,\n",
    "               'bs':64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d3ea22b2-18ad-4994-b1f0-7f904878708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 26ms/step - loss: 1.6287 - balanced_accuracy: 0.2024 - accuracy: 0.1844 - val_loss: 1.6076 - val_balanced_accuracy: 0.2478 - val_accuracy: 0.2299\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.6195 - balanced_accuracy: 0.2100 - accuracy: 0.1993 - val_loss: 1.6065 - val_balanced_accuracy: 0.2127 - val_accuracy: 0.2203\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.6108 - balanced_accuracy: 0.2072 - accuracy: 0.2090 - val_loss: 1.6045 - val_balanced_accuracy: 0.2366 - val_accuracy: 0.2739\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.6106 - balanced_accuracy: 0.2501 - accuracy: 0.2393 - val_loss: 1.6055 - val_balanced_accuracy: 0.2458 - val_accuracy: 0.2452\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.6057 - balanced_accuracy: 0.2243 - accuracy: 0.2140 - val_loss: 1.6042 - val_balanced_accuracy: 0.2675 - val_accuracy: 0.2644\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.6036 - balanced_accuracy: 0.2211 - accuracy: 0.2236 - val_loss: 1.6013 - val_balanced_accuracy: 0.2646 - val_accuracy: 0.2625\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.6044 - balanced_accuracy: 0.2243 - accuracy: 0.2307 - val_loss: 1.5978 - val_balanced_accuracy: 0.2853 - val_accuracy: 0.2816\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.6126 - balanced_accuracy: 0.2333 - accuracy: 0.2392 - val_loss: 1.5957 - val_balanced_accuracy: 0.3178 - val_accuracy: 0.2950\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5957 - balanced_accuracy: 0.2505 - accuracy: 0.2592 - val_loss: 1.5913 - val_balanced_accuracy: 0.3389 - val_accuracy: 0.3295\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5974 - balanced_accuracy: 0.2440 - accuracy: 0.2626 - val_loss: 1.5843 - val_balanced_accuracy: 0.3326 - val_accuracy: 0.3103\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5965 - balanced_accuracy: 0.2408 - accuracy: 0.2496 - val_loss: 1.5803 - val_balanced_accuracy: 0.2888 - val_accuracy: 0.2950\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 1.5834 - balanced_accuracy: 0.2419 - accuracy: 0.2528 - val_loss: 1.5795 - val_balanced_accuracy: 0.3633 - val_accuracy: 0.3429\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 1.6008 - balanced_accuracy: 0.2399 - accuracy: 0.2481 - val_loss: 1.5716 - val_balanced_accuracy: 0.3432 - val_accuracy: 0.3372\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 1.5789 - balanced_accuracy: 0.2623 - accuracy: 0.2811 - val_loss: 1.5722 - val_balanced_accuracy: 0.3413 - val_accuracy: 0.3257\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5862 - balanced_accuracy: 0.2688 - accuracy: 0.2689 - val_loss: 1.5549 - val_balanced_accuracy: 0.3574 - val_accuracy: 0.3429\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5658 - balanced_accuracy: 0.2811 - accuracy: 0.3061 - val_loss: 1.5584 - val_balanced_accuracy: 0.3373 - val_accuracy: 0.3276\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 1.5683 - balanced_accuracy: 0.2661 - accuracy: 0.2725 - val_loss: 1.5609 - val_balanced_accuracy: 0.3159 - val_accuracy: 0.3084\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5792 - balanced_accuracy: 0.2720 - accuracy: 0.2704 - val_loss: 1.5566 - val_balanced_accuracy: 0.3487 - val_accuracy: 0.3372\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 1s 25ms/step - loss: 1.5840 - balanced_accuracy: 0.2657 - accuracy: 0.2748 - val_loss: 1.5440 - val_balanced_accuracy: 0.3518 - val_accuracy: 0.3487\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5714 - balanced_accuracy: 0.2923 - accuracy: 0.2981 - val_loss: 1.5543 - val_balanced_accuracy: 0.3444 - val_accuracy: 0.3276\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5705 - balanced_accuracy: 0.2874 - accuracy: 0.2955 - val_loss: 1.5445 - val_balanced_accuracy: 0.3629 - val_accuracy: 0.3621\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 1.5858 - balanced_accuracy: 0.2544 - accuracy: 0.2606 - val_loss: 1.5436 - val_balanced_accuracy: 0.3785 - val_accuracy: 0.3697\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5613 - balanced_accuracy: 0.2730 - accuracy: 0.2854 - val_loss: 1.5440 - val_balanced_accuracy: 0.3594 - val_accuracy: 0.3506\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5755 - balanced_accuracy: 0.2702 - accuracy: 0.2694 - val_loss: 1.5310 - val_balanced_accuracy: 0.3761 - val_accuracy: 0.3755\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5505 - balanced_accuracy: 0.2838 - accuracy: 0.3074 - val_loss: 1.5281 - val_balanced_accuracy: 0.3806 - val_accuracy: 0.3755\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5584 - balanced_accuracy: 0.2979 - accuracy: 0.3140 - val_loss: 1.5277 - val_balanced_accuracy: 0.3738 - val_accuracy: 0.3736\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5415 - balanced_accuracy: 0.3023 - accuracy: 0.3172 - val_loss: 1.5260 - val_balanced_accuracy: 0.3853 - val_accuracy: 0.3716\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5502 - balanced_accuracy: 0.2965 - accuracy: 0.3043 - val_loss: 1.5220 - val_balanced_accuracy: 0.3810 - val_accuracy: 0.3716\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5640 - balanced_accuracy: 0.2769 - accuracy: 0.2874 - val_loss: 1.5266 - val_balanced_accuracy: 0.3648 - val_accuracy: 0.3659\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 1.5446 - balanced_accuracy: 0.2896 - accuracy: 0.3072 - val_loss: 1.5153 - val_balanced_accuracy: 0.4042 - val_accuracy: 0.3927\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5469 - balanced_accuracy: 0.2988 - accuracy: 0.3111 - val_loss: 1.5190 - val_balanced_accuracy: 0.3619 - val_accuracy: 0.3621\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5398 - balanced_accuracy: 0.2917 - accuracy: 0.3026 - val_loss: 1.5136 - val_balanced_accuracy: 0.3686 - val_accuracy: 0.3697\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5516 - balanced_accuracy: 0.2981 - accuracy: 0.3126 - val_loss: 1.5157 - val_balanced_accuracy: 0.3824 - val_accuracy: 0.3812\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5571 - balanced_accuracy: 0.2845 - accuracy: 0.2975 - val_loss: 1.5208 - val_balanced_accuracy: 0.3942 - val_accuracy: 0.3755\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5307 - balanced_accuracy: 0.3027 - accuracy: 0.3248 - val_loss: 1.5121 - val_balanced_accuracy: 0.3868 - val_accuracy: 0.3831\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5299 - balanced_accuracy: 0.2958 - accuracy: 0.3168 - val_loss: 1.5032 - val_balanced_accuracy: 0.4051 - val_accuracy: 0.3908\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5476 - balanced_accuracy: 0.2941 - accuracy: 0.3135 - val_loss: 1.5059 - val_balanced_accuracy: 0.3933 - val_accuracy: 0.3908\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.5508 - balanced_accuracy: 0.3077 - accuracy: 0.3251 - val_loss: 1.5049 - val_balanced_accuracy: 0.4029 - val_accuracy: 0.4080\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.5458 - balanced_accuracy: 0.2964 - accuracy: 0.3125 - val_loss: 1.5015 - val_balanced_accuracy: 0.3828 - val_accuracy: 0.3812\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5395 - balanced_accuracy: 0.2938 - accuracy: 0.3115 - val_loss: 1.4954 - val_balanced_accuracy: 0.4000 - val_accuracy: 0.4080\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5343 - balanced_accuracy: 0.3110 - accuracy: 0.3271 - val_loss: 1.4982 - val_balanced_accuracy: 0.3923 - val_accuracy: 0.3985\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5302 - balanced_accuracy: 0.3258 - accuracy: 0.3403 - val_loss: 1.5009 - val_balanced_accuracy: 0.4017 - val_accuracy: 0.3908\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5360 - balanced_accuracy: 0.3109 - accuracy: 0.3360 - val_loss: 1.4977 - val_balanced_accuracy: 0.4265 - val_accuracy: 0.4310\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.5275 - balanced_accuracy: 0.2980 - accuracy: 0.3151 - val_loss: 1.4946 - val_balanced_accuracy: 0.3941 - val_accuracy: 0.3889\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5265 - balanced_accuracy: 0.3092 - accuracy: 0.3268 - val_loss: 1.4955 - val_balanced_accuracy: 0.3803 - val_accuracy: 0.3774\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5370 - balanced_accuracy: 0.3064 - accuracy: 0.3239 - val_loss: 1.4956 - val_balanced_accuracy: 0.3985 - val_accuracy: 0.3985\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5082 - balanced_accuracy: 0.3329 - accuracy: 0.3426 - val_loss: 1.4921 - val_balanced_accuracy: 0.4144 - val_accuracy: 0.4061\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5298 - balanced_accuracy: 0.3134 - accuracy: 0.3265 - val_loss: 1.4909 - val_balanced_accuracy: 0.3991 - val_accuracy: 0.4100\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5331 - balanced_accuracy: 0.3179 - accuracy: 0.3337 - val_loss: 1.4933 - val_balanced_accuracy: 0.3867 - val_accuracy: 0.3946\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 1.5363 - balanced_accuracy: 0.3074 - accuracy: 0.3245 - val_loss: 1.4878 - val_balanced_accuracy: 0.3987 - val_accuracy: 0.3985\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5249 - balanced_accuracy: 0.3190 - accuracy: 0.3364 - val_loss: 1.4875 - val_balanced_accuracy: 0.4045 - val_accuracy: 0.4042\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5300 - balanced_accuracy: 0.3233 - accuracy: 0.3381 - val_loss: 1.4929 - val_balanced_accuracy: 0.4120 - val_accuracy: 0.4004\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.5281 - balanced_accuracy: 0.3125 - accuracy: 0.3380 - val_loss: 1.4890 - val_balanced_accuracy: 0.4191 - val_accuracy: 0.4157\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.5389 - balanced_accuracy: 0.3090 - accuracy: 0.3278 - val_loss: 1.4888 - val_balanced_accuracy: 0.3884 - val_accuracy: 0.4080\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5304 - balanced_accuracy: 0.3224 - accuracy: 0.3404 - val_loss: 1.4808 - val_balanced_accuracy: 0.4159 - val_accuracy: 0.4080\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5137 - balanced_accuracy: 0.3142 - accuracy: 0.3434 - val_loss: 1.4957 - val_balanced_accuracy: 0.3846 - val_accuracy: 0.3966\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5125 - balanced_accuracy: 0.3225 - accuracy: 0.3397 - val_loss: 1.4872 - val_balanced_accuracy: 0.4049 - val_accuracy: 0.4042\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 1.5230 - balanced_accuracy: 0.3165 - accuracy: 0.3314 - val_loss: 1.4910 - val_balanced_accuracy: 0.3870 - val_accuracy: 0.4023\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 1.5055 - balanced_accuracy: 0.3278 - accuracy: 0.3464 - val_loss: 1.4842 - val_balanced_accuracy: 0.3859 - val_accuracy: 0.3889\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 1s 23ms/step - loss: 1.5163 - balanced_accuracy: 0.3376 - accuracy: 0.3590 - val_loss: 1.4882 - val_balanced_accuracy: 0.4032 - val_accuracy: 0.4080\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 1.5361 - balanced_accuracy: 0.3206 - accuracy: 0.3304 - val_loss: 1.4834 - val_balanced_accuracy: 0.4115 - val_accuracy: 0.4061\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.5367 - balanced_accuracy: 0.3179 - accuracy: 0.3368 - val_loss: 1.4800 - val_balanced_accuracy: 0.4113 - val_accuracy: 0.4004\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 1s 24ms/step - loss: 1.4839 - balanced_accuracy: 0.3393 - accuracy: 0.3558 - val_loss: 1.4862 - val_balanced_accuracy: 0.4167 - val_accuracy: 0.4215\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 20\n",
    "\n",
    "# Create the EarlyStopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_balanced_accuracy',  # Metric to monitor\n",
    "    patience=early_stopping_patience,  # Number of epochs with no improvement\n",
    "    restore_best_weights=True  # Restore the weights of the best model\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "try:\n",
    "    os.mkdir(f'{path_models}{name_var}')\n",
    "except: pass\n",
    "\n",
    "filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             mode='auto',save_weights_only=False)\n",
    "\n",
    "if dict_params['weighted_loss']==True:\n",
    "\n",
    "    y_train = copy.deepcopy(train_joint_dataset[1])\n",
    "    y_train_integers = np.argmax(y_train, axis=1)\n",
    "    class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(y_train_integers),\n",
    "                                         y = y_train_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_joint_dataset[0][index_random],\n",
    "        train_joint_dataset[1][index_random],\n",
    "        batch_size=dict_params['bs'],\n",
    "        validation_data=val_joint_dataset,\n",
    "        class_weight = d_class_weights,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint,early_stopping_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_joint_dataset[0][index_random],\n",
    "        train_joint_dataset[1][index_random],\n",
    "        batch_size=dict_params['bs'],\n",
    "        validation_data=val_joint_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint,early_stopping_callback],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3807e07e-8395-4500-9fac-60d4ef0d5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 18ms/step - loss: 1.5068 - balanced_accuracy: 0.2900 - accuracy: 0.3627\n",
      "test_balanced_accuracy 0.2900058329105377\n",
      "test_accuracy 0.36274510622024536\n",
      "val_balanced_accuracy 0.42650604248046875\n",
      "val_accuracy 0.43103447556495667\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(test_joint_dataset.batch(len(test_joint_dataset)/5))\n",
    "test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(test_joint_dataset[0],test_joint_dataset[1])\n",
    "# test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(test_joint_dataset)\n",
    "val_balanced_accuracy = np.max(history.history['val_balanced_accuracy'])\n",
    "val_accuracy = np.max(history.history['val_accuracy'])\n",
    "\n",
    "print('test_balanced_accuracy',test_balanced_accuracy)\n",
    "print('test_accuracy',test_accuracy)\n",
    "print('val_balanced_accuracy',val_balanced_accuracy)\n",
    "print('val_accuracy',val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1e90d-3d0a-4cbd-9772-99844d51cc58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a1e39-70f5-4de0-b9c0-9d43b8ddaec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f80d4-eed3-43ff-afdf-a6b15516d336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb607dea-b039-42ae-bebe-887a36ab0260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7cbe1-fa49-4f2f-9bd0-7257117ee297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2a951-da0a-45c8-a379-63238e173f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f98acf-096a-4a8f-b2ae-f381bc3daf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4752cc-c8ac-4b90-8cfc-4c67901d6dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222bc08-5f73-4cc5-9869-0500e01c7155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2bd005-ee76-47cc-931b-7c6e8835756f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "840767ba-4a79-48e6-93f2-837353b931a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "\n",
    "dict_params = {'model_base':'inception',\n",
    "               'type_pooling':'avg',\n",
    "               'do':0.7,\n",
    "               'md':8,\n",
    "               'activation':'LeakyReLU',\n",
    "               'weighted_loss':True,\n",
    "               'bs':32,\n",
    "               'lr':0.001,\n",
    "               'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "\n",
    "# with strategy.scope():\n",
    "if dict_params['model_base']=='resnet50':\n",
    "    model = build_resnet50_model(dict_params['type_pooling'],\n",
    "                                 dict_params['do'],\n",
    "                                 dict_params['md'],\n",
    "                                 dict_params['activation'],\n",
    "                                 dict_params['input_shape'])\n",
    "elif dict_params['model_base']=='resnet101':\n",
    "    model = build_resnet101_model(dict_params['type_pooling'],\n",
    "                                 dict_params['do'],\n",
    "                                 dict_params['md'],\n",
    "                                 dict_params['activation'])\n",
    "elif dict_params['model_base']=='inception':\n",
    "    model = build_inception_model(dict_params['type_pooling'],\n",
    "                                 dict_params['do'],\n",
    "                                 dict_params['md'],\n",
    "                                 dict_params['activation'],\n",
    "                                 dict_params['input_shape'])\n",
    "elif dict_params['model_base']=='xception':\n",
    "    model = build_xception_model(dict_params['type_pooling'],\n",
    "                                 dict_params['do'],\n",
    "                                 dict_params['md'],\n",
    "                                 dict_params['activation'],\n",
    "                                 dict_params['input_shape'])\n",
    "elif dict_params['model_base']=='densenet':\n",
    "    model = build_densenet_model(dict_params['type_pooling'],\n",
    "                                 dict_params['do'],\n",
    "                                 dict_params['md'],\n",
    "                                 dict_params['activation'],\n",
    "                                 dict_params['input_shape'])\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "              metrics=[balanced_accuracy,'accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b6dac61-0650-4779-b258-e195e61852e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "98/98 [==============================] - 33s 294ms/step - loss: 1.8201 - balanced_accuracy: 0.2142 - accuracy: 0.2084 - val_loss: 19.7161 - val_balanced_accuracy: 0.1647 - val_accuracy: 0.1762\n",
      "Epoch 2/100\n",
      "98/98 [==============================] - 28s 283ms/step - loss: 1.6693 - balanced_accuracy: 0.2018 - accuracy: 0.2081 - val_loss: 1.6278 - val_balanced_accuracy: 0.2137 - val_accuracy: 0.1379\n",
      "Epoch 3/100\n",
      "98/98 [==============================] - 28s 284ms/step - loss: 1.6139 - balanced_accuracy: 0.1941 - accuracy: 0.1990 - val_loss: 11.4121 - val_balanced_accuracy: 0.2137 - val_accuracy: 0.1379\n",
      "Epoch 4/100\n",
      "98/98 [==============================] - 28s 285ms/step - loss: 1.6277 - balanced_accuracy: 0.1986 - accuracy: 0.1586 - val_loss: 58.2995 - val_balanced_accuracy: 0.2137 - val_accuracy: 0.2414\n",
      "Epoch 5/100\n",
      "98/98 [==============================] - 28s 285ms/step - loss: 1.6261 - balanced_accuracy: 0.1958 - accuracy: 0.2248 - val_loss: 32.4327 - val_balanced_accuracy: 0.2148 - val_accuracy: 0.1494\n",
      "Epoch 6/100\n",
      "98/98 [==============================] - 28s 288ms/step - loss: 1.6163 - balanced_accuracy: 0.2078 - accuracy: 0.2284 - val_loss: 38.0726 - val_balanced_accuracy: 0.2137 - val_accuracy: 0.2318\n",
      "Epoch 7/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6149 - balanced_accuracy: 0.2099 - accuracy: 0.2035 - val_loss: 21.7987 - val_balanced_accuracy: 0.2137 - val_accuracy: 0.1379\n",
      "Epoch 8/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6152 - balanced_accuracy: 0.2113 - accuracy: 0.2116 - val_loss: 1.6607 - val_balanced_accuracy: 0.1647 - val_accuracy: 0.1762\n",
      "Epoch 9/100\n",
      "98/98 [==============================] - 28s 289ms/step - loss: 1.6155 - balanced_accuracy: 0.2004 - accuracy: 0.1843 - val_loss: 1.6219 - val_balanced_accuracy: 0.2137 - val_accuracy: 0.2318\n",
      "Epoch 10/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6092 - balanced_accuracy: 0.2001 - accuracy: 0.2024 - val_loss: 1.6089 - val_balanced_accuracy: 0.2519 - val_accuracy: 0.2031\n",
      "Epoch 11/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6073 - balanced_accuracy: 0.2013 - accuracy: 0.2103 - val_loss: 1.6384 - val_balanced_accuracy: 0.1512 - val_accuracy: 0.1609\n",
      "Epoch 12/100\n",
      "98/98 [==============================] - 28s 288ms/step - loss: 1.6247 - balanced_accuracy: 0.2130 - accuracy: 0.2004 - val_loss: 1.7094 - val_balanced_accuracy: 0.2134 - val_accuracy: 0.2050\n",
      "Epoch 13/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6119 - balanced_accuracy: 0.1989 - accuracy: 0.1885 - val_loss: 1.6148 - val_balanced_accuracy: 0.1817 - val_accuracy: 0.1897\n",
      "Epoch 14/100\n",
      "98/98 [==============================] - 28s 288ms/step - loss: 1.6097 - balanced_accuracy: 0.2217 - accuracy: 0.2015 - val_loss: 1.6104 - val_balanced_accuracy: 0.2026 - val_accuracy: 0.2069\n",
      "Epoch 15/100\n",
      "98/98 [==============================] - 28s 288ms/step - loss: 1.6078 - balanced_accuracy: 0.2137 - accuracy: 0.2139 - val_loss: 1.6146 - val_balanced_accuracy: 0.1669 - val_accuracy: 0.1801\n",
      "Epoch 16/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6145 - balanced_accuracy: 0.2128 - accuracy: 0.2008 - val_loss: 2.1671 - val_balanced_accuracy: 0.1465 - val_accuracy: 0.1705\n",
      "Epoch 17/100\n",
      "98/98 [==============================] - 28s 288ms/step - loss: 1.6245 - balanced_accuracy: 0.1992 - accuracy: 0.1948 - val_loss: 4.3313 - val_balanced_accuracy: 0.1426 - val_accuracy: 0.1590\n",
      "Epoch 18/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6118 - balanced_accuracy: 0.2089 - accuracy: 0.2115 - val_loss: 1.5986 - val_balanced_accuracy: 0.1931 - val_accuracy: 0.2165\n",
      "Epoch 19/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6059 - balanced_accuracy: 0.2129 - accuracy: 0.2133 - val_loss: 1.6138 - val_balanced_accuracy: 0.2131 - val_accuracy: 0.1954\n",
      "Epoch 20/100\n",
      "98/98 [==============================] - 28s 287ms/step - loss: 1.6071 - balanced_accuracy: 0.2158 - accuracy: 0.2137 - val_loss: 1.6152 - val_balanced_accuracy: 0.2316 - val_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "early_stopping_patience = 10\n",
    "\n",
    "# Create the EarlyStopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_balanced_accuracy',  # Metric to monitor\n",
    "    patience=early_stopping_patience,  # Number of epochs with no improvement\n",
    "    restore_best_weights=True  # Restore the weights of the best model\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "try:\n",
    "    os.mkdir(f'{path_models}{name_var}')\n",
    "except: pass\n",
    "\n",
    "filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                             mode='auto',save_weights_only=False)\n",
    "\n",
    "if dict_params['weighted_loss']==True:\n",
    "\n",
    "    y_train = copy.deepcopy(train_joint_dataset[1])\n",
    "    y_train_integers = np.argmax(y_train, axis=1)\n",
    "    class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(y_train_integers),\n",
    "                                         y = y_train_integers)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_joint_dataset[0],\n",
    "        train_joint_dataset[1],\n",
    "        batch_size=dict_params['bs'],\n",
    "        validation_data=val_joint_dataset,\n",
    "        class_weight = d_class_weights,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint,early_stopping_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_joint_dataset[0],\n",
    "        train_joint_dataset[1],\n",
    "        batch_size=dict_params['bs'],\n",
    "        validation_data=val_joint_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint,early_stopping_callback],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953ce23-ac5a-4434-93b6-e3f8fb4c91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with normalization --0.1808035671710968 (tba)\n",
    "# without normalization --0.20597019791603088 (tba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c391756-54ad-44e0-b050-174f4eb78ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4277\n",
    "# changed densenet to resnet50 *0.4755*\n",
    "# changed resnet50 to resnet101 *0.34*\n",
    "# changed resnet101 to inception *0.6057*\n",
    "# changed inception to xception *0.5473*\n",
    "# changed back to inception, changed pooling from max to average *0.7158*\n",
    "# increased dropout from 0.5 to 0.6 *0.7183*\n",
    "# increased dropout from 0.5 to 0.7 *0.7215*\n",
    "# increased dropout from 0.5 to 0.8 *0.7205*\n",
    "# went back to dropout 0.7, increased md to 32 *0.7280*\n",
    "# decreased md to 8 *0.7345*\n",
    "# decreased md to 4 *0.6819*\n",
    "# went back to md = 8, changed activation from LeakyReLU to ReLU *0.7149*\n",
    "# went back to LeakyReLU, changed batch size to 128 *0.26*\n",
    "# changed batch size to 64 *0.7056*\n",
    "# changed batch size to 16 *0.7226*\n",
    "# changed bs back to 32, changed learning rate to 0.001 - *0.7626*\n",
    "# changed learning rate to 0.01 - *0,22*\n",
    "# changed it back to *0.001*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8bf919c-5224-4c8f-ac83-dc22b89f664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 88ms/step - loss: 1.6047 - balanced_accuracy: 0.2060 - accuracy: 0.2885\n",
      "test_balanced_accuracy 0.20597019791603088\n",
      "test_accuracy 0.28853756189346313\n",
      "val_balanced_accuracy 0.25188928842544556\n",
      "val_accuracy 0.24137930572032928\n"
     ]
    }
   ],
   "source": [
    "# test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(test_joint_dataset.batch(len(test_joint_dataset)/5))\n",
    "test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(test_joint_dataset[0],test_joint_dataset[1])\n",
    "# test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(test_joint_dataset)\n",
    "val_balanced_accuracy = np.max(history.history['val_balanced_accuracy'])\n",
    "val_accuracy = np.max(history.history['val_accuracy'])\n",
    "\n",
    "print('test_balanced_accuracy',test_balanced_accuracy)\n",
    "print('test_accuracy',test_accuracy)\n",
    "print('val_balanced_accuracy',val_balanced_accuracy)\n",
    "print('val_accuracy',val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "095b0e94-3ff1-4004-beb8-344e98fe9518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 240, 720, 1), (None, 5)), types: (tf.float64, tf.float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_joint_dataset.batch(dict_params['bs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68b3706d-f0cc-4723-a583-96816f9ac116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 720, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efddc2da-f954-435a-a897-a390613c4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataset is called 'test_dataset'\n",
    "all_input_data = []\n",
    "all_output_data = []\n",
    "\n",
    "for input_data, output_data in test_joint_dataset:\n",
    "    all_input_data.append(input_data.numpy())  # Convert the Tensor to a NumPy array\n",
    "    all_output_data.append(output_data.numpy())\n",
    "# Convert the list of NumPy arrays to a single NumPy array\n",
    "all_input_data_np = np.array(all_input_data)\n",
    "all_output_data_np = np.array(all_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b505078b-c6f5-43e5-8308-c8178ce5d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19588202, 0.20458117, 0.19893521, 0.19674964, 0.20385192],\n",
       "       [0.19588204, 0.2045812 , 0.19893523, 0.19674964, 0.20385194],\n",
       "       [0.19588202, 0.20458117, 0.19893521, 0.19674964, 0.20385192],\n",
       "       ...,\n",
       "       [0.19588202, 0.20458117, 0.19893521, 0.19674964, 0.20385192],\n",
       "       [0.19588202, 0.20458117, 0.19893521, 0.19674964, 0.20385192],\n",
       "       [0.19588202, 0.20458117, 0.19893521, 0.19674964, 0.20385192]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(all_input_data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "656b6803-db3a-4595-a82f-08424284da10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.46623502],\n",
       "        [1.48863234],\n",
       "        [1.5077076 ],\n",
       "        ...,\n",
       "        [1.38798114],\n",
       "        [1.41586549],\n",
       "        [1.44197845]],\n",
       "\n",
       "       [[1.54025296],\n",
       "        [1.55970751],\n",
       "        [1.57487297],\n",
       "        ...,\n",
       "        [1.46626309],\n",
       "        [1.49324022],\n",
       "        [1.51785905]],\n",
       "\n",
       "       [[1.60578687],\n",
       "        [1.62109075],\n",
       "        [1.63273157],\n",
       "        ...,\n",
       "        [1.54104955],\n",
       "        [1.565385  ],\n",
       "        [1.58693654]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.67280749],\n",
       "        [0.6718265 ],\n",
       "        [0.67083733],\n",
       "        ...,\n",
       "        [0.67573343],\n",
       "        [0.67476368],\n",
       "        [0.67378126]],\n",
       "\n",
       "       [[0.64225224],\n",
       "        [0.64167656],\n",
       "        [0.64109503],\n",
       "        ...,\n",
       "        [0.64397349],\n",
       "        [0.64339685],\n",
       "        [0.64282955]],\n",
       "\n",
       "       [[0.61313806],\n",
       "        [0.61294637],\n",
       "        [0.61276231],\n",
       "        ...,\n",
       "        [0.61369119],\n",
       "        [0.61350686],\n",
       "        [0.61332178]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_data_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6deb8419-b1ff-4450-99ff-0a5caf3cfa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_output_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83f700-d5da-4f43-9e8a-ae4545d0e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc974a40-f1bf-4e81-9996-7f9883a814fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(522, 240, 720, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "656278a0-e335-4fb7-a199-b5946f958427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(np.isfinite(input_data.numpy()[:,:,0])==False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97f059e3-dcbd-4f64-a397-af9725418692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb56418d-0a94-4074-9aac-4d3c3c9d02b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DatasetV2.cardinality of <ShuffleDataset shapes: ((240, 720, 1), (5,)), types: (tf.float64, tf.float32)>>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_joint_dataset.cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba80a65-e44c-4c4e-8275-202b869bd710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aced26-0955-4c2d-b8a4-f710c000be0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cc92f-e35d-43b3-8d03-4033331c86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '/glade/work/jhayron/Data4Predictability/models/CNN_Aug17_2023/v0/'\n",
    "optimizer_direction = 'maximize'\n",
    "number_of_random_points = 30  # random searches to start opt process\n",
    "maximum_time = 0.12*60*60  # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4aa61-337c-4b44-ab96-4e969c916e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    ",name_var,week_out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b3cdc-cd5a-4238-8ee7-a0d0662a0a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73cd7a5-3665-4ca3-b8b0-f640a3ad5f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24ba5b-8323-418e-9dd2-0c3afb6f6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, train_joint_dataset, val_joint_dataset, test_joint_dataset,\n",
    "                 path_models, variable, week):\n",
    "        self.train_joint_dataset = train_joint_dataset\n",
    "        self.val_joint_dataset = val_joint_dataset\n",
    "        self.test_joint_dataset = test_joint_dataset\n",
    "        self.path_models = path_models\n",
    "        self.variable = variable\n",
    "        self.week = week\n",
    " \n",
    "    def __call__(self, trial):    \n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        model_base = trial.suggest_categorical('model_base',['vanilla','resnet50','resnet101',\\\n",
    "                                                             'inception','xception','densenet'])\n",
    "        ks = trial.suggest_categorical('ks',[3,5,7,9,11])\n",
    "        ps = trial.suggest_categorical('ps',[2,4,6,8])\n",
    "        type_pooling = trial.suggest_categorical('type_pooling',[None, 'avg','max'])\n",
    "        stc = trial.suggest_categorical('stc',[1,2,3,4])\n",
    "        stp = trial.suggest_categorical('stp',[1,2,3,4])\n",
    "        do = trial.suggest_categorical('do',[0.3,0.4,0.5])\n",
    "        md = trial.suggest_categorical('md',[2,4,8,16])\n",
    "        nfilters = trial.suggest_categorical('nfilters',[4,8,16,32])\n",
    "        activation = trial.suggest_categorical('activation',['LeakyReLU','ReLU'])\n",
    "        weighted_loss = trial.suggest_categorical('weighted_loss',[True,False])\n",
    "        \n",
    "        dict_params = {'model_base':model_base,\n",
    "                       'ks':ks,\n",
    "                       'ps':ps,\n",
    "                       'type_pooling':type_pooling,\n",
    "                       'stc':stc,\n",
    "                       'stp':stp,\n",
    "                       'do':do,\n",
    "                       'md':md,\n",
    "                       'nfilters':nfilters,\n",
    "                       'activation':activation,\n",
    "                       'weighted_loss':weighted_loss}\n",
    "        print(dict_params)                                      \n",
    "        # instantiate and compile model\n",
    "        if dict_params['model_base']=='vanilla':\n",
    "            model = build_vanilla_cnn(dict_params['ks'],\n",
    "                                      dict_params['ps'],\n",
    "                                      dict_params['type_pooling'],\n",
    "                                      dict_params['stc'],\n",
    "                                      dict_params['stp'],\n",
    "                                      dict_params['do'],\n",
    "                                      dict_params['md'],\n",
    "                                      dict_params['nfilters'],\n",
    "                                      dict_params['activation'])\n",
    "        elif dict_params['model_base']=='resnet50':\n",
    "            model = build_resnet50_model(dict_params['type_pooling'],\n",
    "                                         dict_params['do'],\n",
    "                                         dict_params['md'],\n",
    "                                         dict_params['activation'])\n",
    "        elif dict_params['model_base']=='resnet101':\n",
    "            model = build_resnet101_model(dict_params['type_pooling'],\n",
    "                                         dict_params['do'],\n",
    "                                         dict_params['md'],\n",
    "                                         dict_params['activation'])\n",
    "        elif dict_params['model_base']=='inception':\n",
    "            model = build_inception_model(dict_params['type_pooling'],\n",
    "                                         dict_params['do'],\n",
    "                                         dict_params['md'],\n",
    "                                         dict_params['activation'])\n",
    "        elif dict_params['model_base']=='xception':\n",
    "            model = build_xception_model(dict_params['type_pooling'],\n",
    "                                         dict_params['do'],\n",
    "                                         dict_params['md'],\n",
    "                                         dict_params['activation'])\n",
    "        elif dict_params['model_base']=='densenet':\n",
    "            model = build_densenet_model(dict_params['type_pooling'],\n",
    "                                         dict_params['do'],\n",
    "                                         dict_params['md'],\n",
    "                                         dict_params['activation'])\n",
    "            \n",
    "        model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[balanced_accuracy,'accuracy'])\n",
    "        \n",
    "        epochs = 100\n",
    "        early_stopping_patience = 5\n",
    "\n",
    "        # Create the EarlyStopping callback\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_balanced_accuracy',  # Metric to monitor\n",
    "            patience=early_stopping_patience,  # Number of epochs with no improvement\n",
    "            restore_best_weights=True  # Restore the weights of the best model\n",
    "        )\n",
    "\n",
    "        # Train the model with early stopping\n",
    "        try:\n",
    "            os.mkdir(f'{self.path_models}{self.variable}')\n",
    "        except: pass\n",
    "    \n",
    "        filepath = f'{self.path_models}{self.variable}/model_{self.week}_v0.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                     mode='auto',save_weights_only=False)\n",
    "        \n",
    "        if dict_params['weighted_loss']==True:\n",
    "            \n",
    "            y_train = get_output_from_dataset(self.train_joint_dataset)\n",
    "            y_train_integers = np.argmax(y_train, axis=1)\n",
    "            class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(y_train_integers),\n",
    "                                                 y = y_train_integers)\n",
    "            d_class_weights = dict(enumerate(class_weights))\n",
    "            \n",
    "            history = model.fit(\n",
    "                self.train_joint_dataset.batch(32),\n",
    "                validation_data=self.val_joint_dataset.batch(32),\n",
    "                class_weight = d_class_weights,\n",
    "                epochs=epochs,\n",
    "                callbacks=[checkpoint,early_stopping_callback],\n",
    "                verbose=0\n",
    "            )\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                self.train_joint_dataset.batch(32),\n",
    "                validation_data=self.val_joint_dataset.batch(32),\n",
    "                epochs=epochs,\n",
    "                callbacks=[checkpoint,early_stopping_callback],\n",
    "                verbose=0\n",
    "            )\n",
    "        \n",
    "        test_loss, test_balanced_accuracy, test_accuracy = model.evaluate(self.test_joint_dataset.batch(32))\n",
    "        val_balanced_accuracy = np.max(history.history['val_balanced_accuracy'])\n",
    "        val_accuracy = np.max(history.history['val_accuracy'])\n",
    "        \n",
    "        trial.set_user_attr('test_balanced_accuracy',test_balanced_accuracy)\n",
    "        trial.set_user_attr('test_accuracy',test_accuracy)\n",
    "        trial.set_user_attr('val_balanced_accuracy',val_balanced_accuracy)\n",
    "        trial.set_user_attr('val_accuracy',val_accuracy)\n",
    "        \n",
    "        return val_balanced_accuracy\n",
    "    \n",
    "    \n",
    "# Get variable and origin from command-line arguments\n",
    "name_var = sys.argv[1]\n",
    "week_out = sys.argv[2]\n",
    "path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms/'\n",
    "input_anoms = xr.open_dataset(f'{path_weekly_anoms}{name_var}.nc')\n",
    "var_name = list(input_anoms.data_vars.keys())[0]\n",
    "week_out_str = f'week{week_out}'\n",
    "\n",
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)\n",
    "        \n",
    "train_joint_dataset, val_joint_dataset, test_joint_dataset = \\\n",
    "    create_datasets(input_anoms, var_name, df_shifts, week_out)\n",
    "path_models = '/glade/work/jhayron/Data4Predictability/models/CNN/v0/'\n",
    "optimizer_direction = 'maximize'\n",
    "number_of_random_points = 30  # random searches to start opt process\n",
    "maximum_time = 0.12*60*60  # seconds\n",
    "objective = Objective(train_joint_dataset,val_joint_dataset,test_joint_dataset,\n",
    "                      path_models,name_var,week_out_str)\n",
    "    \n",
    "results_directory = f'/glade/work/jhayron/Data4Predictability/models/CNN/results_optuna/{week_out_str}/'\n",
    "try:\n",
    "    os.mkdir(results_directory)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "study_name = f'study_{name_var}_{week_out_str}_v0'\n",
    "storage_name = f'sqlite:///{study_name}.db'\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction=optimizer_direction,\n",
    "        sampler=TPESampler(n_startup_trials=number_of_random_points),\n",
    "        study_name=study_name, storage=storage_name,load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, timeout=maximum_time, gc_after_trial=True,callbacks=[logging_callback],)\n",
    "\n",
    "# save results\n",
    "df_results = study.trials_dataframe()\n",
    "df_results.to_pickle(results_directory + f'df_optuna_results_{name_var}_v0.pkl')\n",
    "df_results.to_csv(results_directory + f'df_optuna_results_{name_var}_v0.csv')\n",
    "#save study\n",
    "joblib.dump(study, results_directory + f'optuna_study_{name_var}_v0.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_wr]",
   "language": "python",
   "name": "conda-env-cnn_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

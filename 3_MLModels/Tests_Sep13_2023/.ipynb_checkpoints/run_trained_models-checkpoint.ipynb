{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c99eff4-125a-49ef-afaf-558b0df7144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 14:25:30.923803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "from keras.utils import to_categorical\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3de435-d5d1-4c9f-a3ae-8cb02b3823d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/glade/u/home/jhayron/WR_Predictability/3_MLModels/\")\n",
    "from model_builders_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c43fd-d289-4bab-b0e0-7a67ddcc3861",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65e0089-54e7-4056-9265-2504ea04161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tf_datasets(input_data, output_data):\n",
    "    # Convert xarray dataset to numpy array for TensorFlow Dataset\n",
    "    input_images = input_data.transpose('time', 'lat', 'lon','channel').values\n",
    "    output_one_hot = output_data.values\n",
    "\n",
    "    # Create TensorFlow Datasets\n",
    "    input_dataset = tf.data.Dataset.from_tensor_slices(input_images)\n",
    "    output_dataset = tf.data.Dataset.from_tensor_slices(output_one_hot)\n",
    "\n",
    "    # Combine input and output datasets into a joint dataset\n",
    "    joint_dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\n",
    "\n",
    "    return (input_images,output_one_hot)\n",
    "\n",
    "def create_datasets(input_anoms, var_name, df_shifts, week_out):\n",
    "# Assuming you have the xarray.Dataset 'input_data' and the pandas.Series 'output_data'\n",
    "    input_data = copy.deepcopy(input_anoms[var_name])\n",
    "\n",
    "    array_temp = input_data.data\n",
    "    array_temp[np.isfinite(array_temp)==False]=0\n",
    "    input_data.data = array_temp\n",
    "\n",
    "#     input_data = (input_data - input_data.mean('time')) / (input_data.std('time'))\n",
    "    \n",
    "#     input_data[np.isfinite(array_temp)==False] = 0\n",
    "    \n",
    "    # Reshape the data to add a new dimension\n",
    "    values_reshaped = input_data.values.reshape(input_data.shape[0], input_data.shape[1], input_data.shape[2], 1)\n",
    "\n",
    "    # Create a new xarray.DataArray with the reshaped data and the original coordinates\n",
    "    input_data = xr.DataArray(values_reshaped, coords=input_data.coords, dims=('time', 'lat', 'lon', 'channel'))\n",
    "    output_data = copy.deepcopy(df_shifts[f'week{week_out}']).dropna()\n",
    "\n",
    "    # Step 1: Create a common date index that includes all dates in both the input and output data\n",
    "    common_dates = np.intersect1d(input_data['time'].values, output_data.index)\n",
    "\n",
    "    # Step 2: Reindex the input xarray dataset and the output DataFrame to the common date index\n",
    "    input_data = input_data.sel(time=common_dates)\n",
    "    output_data = output_data.loc[common_dates]\n",
    "\n",
    "    # Step 3: One-hot encode the output DataFrame using to_categorical\n",
    "    num_classes = len(output_data.unique())  # Number of classes (number of weeks in this case)\n",
    "    output_data_encoded = to_categorical(output_data, num_classes=num_classes)\n",
    "    output_data_encoded = pd.DataFrame(output_data_encoded,index=output_data.index)\n",
    "\n",
    "    # Step 4: Create masks for training, validation, and testing periods\n",
    "    train_mask = (output_data.index >= '1980-01-01') & (output_data.index <= '2010-12-31')\n",
    "    val_mask = (output_data.index >= '2011-01-01') & (output_data.index <= '2015-12-31')\n",
    "    test_mask = (output_data.index >= '2016-01-01') & (output_data.index <= '2020-12-31')\n",
    "\n",
    "    # Step 5: Split the input xarray dataset and the output DataFrame into subsets\n",
    "    input_train = input_data.sel(time=train_mask)\n",
    "    input_val = input_data.sel(time=val_mask)\n",
    "    input_test = input_data.sel(time=test_mask)\n",
    "\n",
    "    output_train = output_data_encoded.loc[train_mask]\n",
    "    output_val = output_data_encoded.loc[val_mask]\n",
    "    output_test = output_data_encoded.loc[test_mask]\n",
    "\n",
    "    train_joint_dataset = create_tf_datasets(input_train, output_train)\n",
    "    val_joint_dataset = create_tf_datasets(input_val, output_val)\n",
    "    test_joint_dataset = create_tf_datasets(input_test, output_test)\n",
    "\n",
    "    # buffer_size = train_joint_dataset.cardinality()\n",
    "    # train_joint_dataset = train_joint_dataset.shuffle(buffer_size)\n",
    "    return train_joint_dataset, val_joint_dataset, test_joint_dataset\n",
    "\n",
    "def get_output_from_dataset(dataset):\n",
    "    output_array = []\n",
    "    for input_data, output_data in dataset.as_numpy_iterator():\n",
    "        output_array.append(output_data)\n",
    "\n",
    "    # Convert the list of NumPy arrays into a single NumPy array\n",
    "    output_array = np.array(output_array)\n",
    "    return output_array\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    return tf.py_function(balanced_accuracy_score, (y_true, y_pred), tf.float32)\n",
    "\n",
    "def logging_callback(study, frozen_trial):\n",
    "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "    if previous_best_value != study.best_value:\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "        print(\n",
    "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
    "            frozen_trial.number,\n",
    "            frozen_trial.value,\n",
    "            frozen_trial.params,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26d5758-dfcf-475f-ab31-1b2e14c0c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_multichannel(input_data, df_shifts, week_out):\n",
    "# Assuming you have the xarray.Dataset 'input_data' and the pandas.Series 'output_data'\n",
    "\n",
    "    # Create a new xarray.DataArray with the reshaped data and the original coordinates\n",
    "    output_data = copy.deepcopy(df_shifts[f'week{week_out}']).dropna()\n",
    "\n",
    "    # Step 1: Create a common date index that includes all dates in both the input and output data\n",
    "    common_dates = np.intersect1d(input_data['time'].values, output_data.index)\n",
    "\n",
    "    # Step 2: Reindex the input xarray dataset and the output DataFrame to the common date index\n",
    "    input_data = input_data.sel(time=common_dates)\n",
    "    output_data = output_data.loc[common_dates]\n",
    "\n",
    "    # Step 3: One-hot encode the output DataFrame using to_categorical\n",
    "    num_classes = len(output_data.unique())  # Number of classes (number of weeks in this case)\n",
    "    output_data_encoded = to_categorical(output_data, num_classes=num_classes)\n",
    "    output_data_encoded = pd.DataFrame(output_data_encoded,index=output_data.index)\n",
    "\n",
    "    # Step 4: Create masks for training, validation, and testing periods\n",
    "    train_mask = (output_data.index >= '1980-01-01') & (output_data.index <= '2010-12-31')\n",
    "    val_mask = (output_data.index >= '2011-01-01') & (output_data.index <= '2015-12-31')\n",
    "    test_mask = (output_data.index >= '2016-01-01') & (output_data.index <= '2020-12-31')\n",
    "\n",
    "    # Step 5: Split the input xarray dataset and the output DataFrame into subsets\n",
    "    input_train = input_data.sel(time=train_mask)\n",
    "    input_val = input_data.sel(time=val_mask)\n",
    "    input_test = input_data.sel(time=test_mask)\n",
    "\n",
    "    output_train = output_data_encoded.loc[train_mask]\n",
    "    output_val = output_data_encoded.loc[val_mask]\n",
    "    output_test = output_data_encoded.loc[test_mask]\n",
    "\n",
    "    train_joint_dataset = create_tf_datasets(input_train, output_train)\n",
    "    val_joint_dataset = create_tf_datasets(input_val, output_val)\n",
    "    test_joint_dataset = create_tf_datasets(input_test, output_test)\n",
    "\n",
    "    return train_joint_dataset, val_joint_dataset, test_joint_dataset,input_train.time,\\\n",
    "        input_val.time,input_test.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0ddb5b-ddc0-4a0e-8c68-9f1946d2b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_with_scaling(data, start_year, end_year):\n",
    "    # Define a normalization function\n",
    "    def normalize_pixel(pixel):\n",
    "        return (pixel - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Create an empty DataArray to store the scaling parameters (min and max) for each pixel and channel\n",
    "    scaling_params = xr.DataArray(np.nan, dims=(\"lat\", \"lon\", \"channel\", \"parameter\"), \n",
    "                                  coords={\"lat\": data.lat, \"lon\": data.lon, \n",
    "                                          \"channel\": data.channel, \"parameter\": [\"min\", \"max\"]})\n",
    "\n",
    "    # Normalize the data using the MinMaxScalers and scaling parameters\n",
    "    normalized_data = data.copy()\n",
    "\n",
    "    # Initialize a dictionary to store MinMaxScalers for each pixel and channel\n",
    "    for channel in data.channel:\n",
    "        data_channel = data.sel(channel=channel)\n",
    "        years_for_scaling = data_channel.sel(time=slice(f\"{start_year}-01-01\", f\"{end_year}-12-31\"))\n",
    "        # Define the min and max values for normalization\n",
    "        min_value = years_for_scaling.min(dim='time')\n",
    "        max_value = years_for_scaling.max(dim='time')\n",
    "        \n",
    "        # Apply the normalization function to each pixel along the 'time' dimension\n",
    "        normalized_data_channel = normalize_pixel(data_channel).values\n",
    "        normalized_data_channel[np.isfinite(normalized_data_channel)==False]=0\n",
    "        normalized_data.loc[dict(channel=channel)] = normalized_data_channel\n",
    "        \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c479563a-1e9e-4926-b735-a3ea74f46c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 14:25:36.984291: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-09-13 14:25:36.985762: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-09-13 14:25:37.027277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:88:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-09-13 14:25:37.027320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-09-13 14:25:37.181293: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-09-13 14:25:37.181379: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-09-13 14:25:37.239852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-09-13 14:25:37.295736: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-09-13 14:25:37.425483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-09-13 14:25:37.494627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-09-13 14:25:37.534209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-09-13 14:25:37.535473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "## GLOBAL SEED ##    \n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989471a-eaaa-43c5-8f51-599709071288",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79725cbe-16a5-4c77-aea7-fc723a9c3933",
   "metadata": {},
   "source": [
    "# olr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafd86b-e251-4679-84d1-00e6525bac7f",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabae911-a169-45d6-bf3d-68fbbbf2fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'atm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef18836c-dbba-4bcc-92ad-d2020305d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}_olr/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed38455-b2cc-4d52-94ed-8d69edb6b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe369097-9835-46bc-8b01-11a025074142",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a84b1b-e712-42a2-a823-ac0d24541586",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7450c70-6bfe-4243-a444-e75e2b86cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 14:26:40.680109: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-13 14:26:40.680294: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-09-13 14:26:40.681393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:88:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-09-13 14:26:40.681475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-09-13 14:26:40.681521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-09-13 14:26:40.681538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-09-13 14:26:40.681553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-09-13 14:26:40.681567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-09-13 14:26:40.681610: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-09-13 14:26:40.681627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-09-13 14:26:40.681642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-09-13 14:26:40.683249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-09-13 14:26:40.683319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-09-13 14:26:41.309704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-09-13 14:26:41.309744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-09-13 14:26:41.309755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-09-13 14:26:41.311485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:88:00.0, compute capability: 7.0)\n",
      "2023-09-13 14:26:47.613436: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-09-13 14:26:47.613978: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n",
      "2023-09-13 14:26:49.729098: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-09-13 14:26:49.957840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    }
   ],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data.sel(channel=[1,5]), df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/atm_olr_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7dca0-1983-4e42-8c71-4a708e5f0b01",
   "metadata": {},
   "source": [
    "# swvl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7317b3-2412-47b1-97a0-9bf2172f23bd",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0012ee66-6bad-4b5c-8edc-c7beddacf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'lnd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d525455-ad72-42a4-85a9-e94c27e1035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}_swvl/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf737e00-3465-4cea-adb5-0f3b06c6f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c5c8f4-7cab-4d86-aeb5-2dcffbdc2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e970f017-0adb-4dd6-9e9d-2c3af40f6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19298b51-87ef-4a64-9ef3-53dc6ffb53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data.sel(channel=[1,5]), df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/lnd_swvl_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8426cfa-38d3-4925-b994-065a59855cbd",
   "metadata": {},
   "source": [
    "# sst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60d569-a6a7-417c-bbfc-597dfc1e8013",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fc34a9b-ac0c-4204-90b0-054ecb30d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'ocn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c46717e-28b6-4cb3-ac24-35e5aede3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}_sst/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61ac8b69-8f86-46e1-bfe2-b6a25c5676d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9d040f4-3fdb-4c1a-aed8-1ca6c24cbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1302b858-e768-4aae-a740-55f0ba4cadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc20586-9d0a-4e3c-95c7-5835a54f62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data.sel(channel=[2,6]), df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/ocn_sst_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145d78e-9001-45e9-a57a-8cfe0eeda0b7",
   "metadata": {},
   "source": [
    "# z_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757aee2-98cc-4c88-9cb4-5d2fe1c7c2ea",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a474bf30-31c7-48a5-8b1e-56ba102ee8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'atm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0406235-c4d6-4bda-8efc-e3132d9ae7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}_z/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "858300fc-6970-4c7f-9d44-f1e7ffdbd19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0397fc8-0b10-4bd9-8dae-708c52e3835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4817cb0-9cff-4310-9407-f5782129ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3dd281f-1d64-4d95-8b62-1a2df2af69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data.sel(channel=[0,4]), df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/atm_z_only_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e8589c-22ee-4d74-93e7-b64d0de64cbb",
   "metadata": {},
   "source": [
    "# atm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12a422-ac11-49c2-89a1-129ed53025ed",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbd733ff-f9cf-48da-b915-20465f0f5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'atm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bebee54-26fc-42f9-9378-238ef0366a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65111bec-7570-41a7-8f8b-cffa79297aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c887b707-686f-47c7-908e-f90f90848e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ba8c84e-549e-47db-b07b-4aa7a099b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbdd3e-b5c7-4100-b6db-042137dd59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data, df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/{name_var}_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98771306-378a-4abd-9366-84d2dd932174",
   "metadata": {},
   "source": [
    "# land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba41feb-713d-4646-a626-9a798a2ad3c5",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1743b-9a7a-4c74-a15e-b934c3f1a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'lnd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6f866-951a-42cd-8a9a-b2787dd30a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db93796-df0b-45c3-b88d-e072d789dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715b87c-74ed-4567-aa31-e1217a64dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc529e-f097-48d3-a68c-ba09c3bad0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e10cccd-8a7c-4ee6-8447-dab690a9db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data, df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/{name_var}_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb08ec7-8a6a-4fc3-bcaf-66fa1fe4c7f7",
   "metadata": {},
   "source": [
    "# ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f6ea8a-2ff2-4146-9ae9-2c1558801602",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c8bf68e-fb5a-4035-b68a-690d4c85e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_var = 'ocn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9d3cfad-cf7e-4108-8f32-ea301d4c655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f'/glade/work/jhayron/Data4Predictability/models/CNN_Sep13_2023/test_{name_var}/'\n",
    "try:\n",
    "    os.mkdir(path_models)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96944390-399a-41df-9d77-676aeab100fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = xr.open_dataset(f'/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/{name_var}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21d9ad58-4126-4b6f-a411-4598c306a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b366387b-fb26-49b9-8e86-41c65a111103",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbac1a50-6f6d-4d21-a16c-f86f1581aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for week_out in range(0,9):\n",
    "    week_out_str = f'week{week_out}'\n",
    "    \n",
    "    train_joint_dataset, val_joint_dataset, test_joint_dataset, _, _, test_index  = \\\n",
    "        create_datasets_multichannel(normalized_data, df_shifts, week_out)\n",
    "    \n",
    "    dict_params = {'model_base':'densenet',\n",
    "                   'type_pooling':'avg',\n",
    "                   'do':0.3,\n",
    "                   'md':16,\n",
    "                   'activation':'LeakyReLU',\n",
    "                   'weighted_loss':True,\n",
    "                   'bs':16,\n",
    "                   'lr':0.001,\n",
    "                   'input_shape':train_joint_dataset[0].shape[1:]}\n",
    "    # with strategy.scope():\n",
    "    model = build_predesigned_model(dict_params['model_base'],\n",
    "                                    dict_params['type_pooling'],\n",
    "                                    dict_params['do'],\n",
    "                                    dict_params['md'],\n",
    "                                    dict_params['activation'],\n",
    "                                    dict_params['input_shape'])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=keras.optimizers.Adam(lr=dict_params['lr']),\n",
    "                  metrics=[balanced_accuracy,'accuracy'])  \n",
    "    \n",
    "    filepath = f'{path_models}{name_var}/model_{week_out_str}_v0.h5'\n",
    "\n",
    "    model.load_weights(filepath)\n",
    "    predictions_prob = model.predict(test_joint_dataset[0])\n",
    "    df_results = pd.DataFrame(predictions_prob,index=test_index)\n",
    "    df_results.to_csv(f'results/{name_var}_{week_out_str}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db770d8c-7d23-4a6d-a59e-c9487bd5c2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_wr]",
   "language": "python",
   "name": "conda-env-cnn_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c99eff4-125a-49ef-afaf-558b0df7144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 23:23:14.258544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "from keras.utils import to_categorical\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3de435-d5d1-4c9f-a3ae-8cb02b3823d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/glade/u/home/jhayron/WR_Predictability/3_MLModels/\")\n",
    "from model_builders_v2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3c43fd-d289-4bab-b0e0-7a67ddcc3861",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65e0089-54e7-4056-9265-2504ea04161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_tf_datasets(input_data, output_data):\n",
    "#     # Convert xarray dataset to numpy array for TensorFlow Dataset\n",
    "#     input_images = input_data.transpose('time', 'lat', 'lon','channel').values\n",
    "#     output_one_hot = output_data.values\n",
    "\n",
    "#     # Create TensorFlow Datasets\n",
    "#     input_dataset = tf.data.Dataset.from_tensor_slices(input_images)\n",
    "#     output_dataset = tf.data.Dataset.from_tensor_slices(output_one_hot)\n",
    "\n",
    "#     # Combine input and output datasets into a joint dataset\n",
    "#     joint_dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\n",
    "\n",
    "#     return joint_dataset\n",
    "def create_tf_datasets(input_data, output_data):\n",
    "    # Convert xarray dataset to numpy array for TensorFlow Dataset\n",
    "    input_images = input_data.transpose('time', 'lat', 'lon','channel').values\n",
    "    output_one_hot = output_data.values\n",
    "\n",
    "    # Create TensorFlow Datasets\n",
    "    input_dataset = tf.data.Dataset.from_tensor_slices(input_images)\n",
    "    output_dataset = tf.data.Dataset.from_tensor_slices(output_one_hot)\n",
    "\n",
    "    # Combine input and output datasets into a joint dataset\n",
    "    joint_dataset = tf.data.Dataset.zip((input_dataset, output_dataset))\n",
    "\n",
    "    return (input_images,output_one_hot)\n",
    "\n",
    "def create_datasets(input_anoms, var_name, df_shifts, week_out):\n",
    "# Assuming you have the xarray.Dataset 'input_data' and the pandas.Series 'output_data'\n",
    "    input_data = copy.deepcopy(input_anoms[var_name])\n",
    "\n",
    "    array_temp = input_data.data\n",
    "    array_temp[np.isfinite(array_temp)==False]=0\n",
    "    input_data.data = array_temp\n",
    "\n",
    "#     input_data = (input_data - input_data.mean('time')) / (input_data.std('time'))\n",
    "    \n",
    "#     input_data[np.isfinite(array_temp)==False] = 0\n",
    "    \n",
    "    # Reshape the data to add a new dimension\n",
    "    values_reshaped = input_data.values.reshape(input_data.shape[0], input_data.shape[1], input_data.shape[2], 1)\n",
    "\n",
    "    # Create a new xarray.DataArray with the reshaped data and the original coordinates\n",
    "    input_data = xr.DataArray(values_reshaped, coords=input_data.coords, dims=('time', 'lat', 'lon', 'channel'))\n",
    "    output_data = copy.deepcopy(df_shifts[f'week{week_out}']).dropna()\n",
    "\n",
    "    # Step 1: Create a common date index that includes all dates in both the input and output data\n",
    "    common_dates = np.intersect1d(input_data['time'].values, output_data.index)\n",
    "\n",
    "    # Step 2: Reindex the input xarray dataset and the output DataFrame to the common date index\n",
    "    input_data = input_data.sel(time=common_dates)\n",
    "    output_data = output_data.loc[common_dates]\n",
    "\n",
    "    # Step 3: One-hot encode the output DataFrame using to_categorical\n",
    "    num_classes = len(output_data.unique())  # Number of classes (number of weeks in this case)\n",
    "    output_data_encoded = to_categorical(output_data, num_classes=num_classes)\n",
    "    output_data_encoded = pd.DataFrame(output_data_encoded,index=output_data.index)\n",
    "\n",
    "    # Step 4: Create masks for training, validation, and testing periods\n",
    "    train_mask = (output_data.index >= '1980-01-01') & (output_data.index <= '2010-12-31')\n",
    "    val_mask = (output_data.index >= '2011-01-01') & (output_data.index <= '2015-12-31')\n",
    "    test_mask = (output_data.index >= '2016-01-01') & (output_data.index <= '2020-12-31')\n",
    "\n",
    "    # Step 5: Split the input xarray dataset and the output DataFrame into subsets\n",
    "    input_train = input_data.sel(time=train_mask)\n",
    "    input_val = input_data.sel(time=val_mask)\n",
    "    input_test = input_data.sel(time=test_mask)\n",
    "\n",
    "    output_train = output_data_encoded.loc[train_mask]\n",
    "    output_val = output_data_encoded.loc[val_mask]\n",
    "    output_test = output_data_encoded.loc[test_mask]\n",
    "\n",
    "    train_joint_dataset = create_tf_datasets(input_train, output_train)\n",
    "    val_joint_dataset = create_tf_datasets(input_val, output_val)\n",
    "    test_joint_dataset = create_tf_datasets(input_test, output_test)\n",
    "\n",
    "    # buffer_size = train_joint_dataset.cardinality()\n",
    "    # train_joint_dataset = train_joint_dataset.shuffle(buffer_size)\n",
    "    return train_joint_dataset, val_joint_dataset, test_joint_dataset\n",
    "\n",
    "def get_output_from_dataset(dataset):\n",
    "    output_array = []\n",
    "    for input_data, output_data in dataset.as_numpy_iterator():\n",
    "        output_array.append(output_data)\n",
    "\n",
    "    # Convert the list of NumPy arrays into a single NumPy array\n",
    "    output_array = np.array(output_array)\n",
    "    return output_array\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    y_true = tf.argmax(y_true, axis=1)\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    return tf.py_function(balanced_accuracy_score, (y_true, y_pred), tf.float32)\n",
    "\n",
    "def logging_callback(study, frozen_trial):\n",
    "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "    if previous_best_value != study.best_value:\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "        print(\n",
    "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
    "            frozen_trial.number,\n",
    "            frozen_trial.value,\n",
    "            frozen_trial.params,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26d5758-dfcf-475f-ab31-1b2e14c0c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_multichannel(input_data, df_shifts, week_out):\n",
    "# Assuming you have the xarray.Dataset 'input_data' and the pandas.Series 'output_data'\n",
    "\n",
    "    # Create a new xarray.DataArray with the reshaped data and the original coordinates\n",
    "    output_data = copy.deepcopy(df_shifts[f'week{week_out}']).dropna()\n",
    "\n",
    "    # Step 1: Create a common date index that includes all dates in both the input and output data\n",
    "    common_dates = np.intersect1d(input_data['time'].values, output_data.index)\n",
    "\n",
    "    # Step 2: Reindex the input xarray dataset and the output DataFrame to the common date index\n",
    "    input_data = input_data.sel(time=common_dates)\n",
    "    output_data = output_data.loc[common_dates]\n",
    "\n",
    "    # Step 3: One-hot encode the output DataFrame using to_categorical\n",
    "    num_classes = len(output_data.unique())  # Number of classes (number of weeks in this case)\n",
    "    output_data_encoded = to_categorical(output_data, num_classes=num_classes)\n",
    "    output_data_encoded = pd.DataFrame(output_data_encoded,index=output_data.index)\n",
    "\n",
    "    # Step 4: Create masks for training, validation, and testing periods\n",
    "    train_mask = (output_data.index >= '1980-01-01') & (output_data.index <= '2010-12-31')\n",
    "    val_mask = (output_data.index >= '2011-01-01') & (output_data.index <= '2015-12-31')\n",
    "    test_mask = (output_data.index >= '2016-01-01') & (output_data.index <= '2020-12-31')\n",
    "\n",
    "    # Step 5: Split the input xarray dataset and the output DataFrame into subsets\n",
    "    input_train = input_data.sel(time=train_mask)\n",
    "    input_val = input_data.sel(time=val_mask)\n",
    "    input_test = input_data.sel(time=test_mask)\n",
    "\n",
    "    output_train = output_data_encoded.loc[train_mask]\n",
    "    output_val = output_data_encoded.loc[val_mask]\n",
    "    output_test = output_data_encoded.loc[test_mask]\n",
    "\n",
    "    train_joint_dataset = create_tf_datasets(input_train, output_train)\n",
    "    val_joint_dataset = create_tf_datasets(input_val, output_val)\n",
    "    test_joint_dataset = create_tf_datasets(input_test, output_test)\n",
    "\n",
    "    return train_joint_dataset, val_joint_dataset, test_joint_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0ddb5b-ddc0-4a0e-8c68-9f1946d2b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_with_scaling(data, start_year, end_year):\n",
    "    # Define a normalization function\n",
    "    def normalize_pixel(pixel):\n",
    "        return (pixel - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Create an empty DataArray to store the scaling parameters (min and max) for each pixel and channel\n",
    "    scaling_params = xr.DataArray(np.nan, dims=(\"lat\", \"lon\", \"channel\", \"parameter\"), \n",
    "                                  coords={\"lat\": data.lat, \"lon\": data.lon, \n",
    "                                          \"channel\": data.channel, \"parameter\": [\"min\", \"max\"]})\n",
    "\n",
    "    # Normalize the data using the MinMaxScalers and scaling parameters\n",
    "    normalized_data = data.copy()\n",
    "\n",
    "    # Initialize a dictionary to store MinMaxScalers for each pixel and channel\n",
    "    for channel in data.channel:\n",
    "        data_channel = data.sel(channel=channel)\n",
    "        years_for_scaling = data_channel.sel(time=slice(f\"{start_year}-01-01\", f\"{end_year}-12-31\"))\n",
    "        # Define the min and max values for normalization\n",
    "        min_value = years_for_scaling.min(dim='time')\n",
    "        max_value = years_for_scaling.max(dim='time')\n",
    "        \n",
    "        # Apply the normalization function to each pixel along the 'time' dimension\n",
    "        normalized_data_channel = normalize_pixel(data_channel).values\n",
    "        normalized_data_channel[np.isfinite(normalized_data_channel)==False]=0\n",
    "        normalized_data.loc[dict(channel=channel)] = normalized_data_channel\n",
    "        \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafd86b-e251-4679-84d1-00e6525bac7f",
   "metadata": {},
   "source": [
    "IC_SODA.nc   OHC100_SODA.nc  OHC50_SODA.nc   SD_ERA5.nc      SST_SODA.nc       STL_7cm_ERA5.nc   SWVL_28cm_ERA5.nc  U10_ERA5.nc\n",
    "IT_SODA.nc   OHC200_SODA.nc  OHC700_SODA.nc  SSH_SODA.nc     STL_1m_ERA5.nc    STL_full_ERA5.nc  SWVL_7cm_ERA5.nc   U200_ERA5.nc\n",
    "MLD_SODA.nc  OHC300_SODA.nc  OLR_ERA5.nc     SST_OISSTv2.nc  STL_28cm_ERA5.nc  SWVL_1m_ERA5.nc   SWVL_full_ERA5.nc  Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98944495-f7bb-48fd-920f-4b2423ae96c8",
   "metadata": {},
   "source": [
    "# Atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5d8577-3dbe-4062-9a39-f63fc66ef446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## array_anoms\n",
    "names_vars = ['Z500_ERA5','OLR_ERA5','U200_ERA5','U10_ERA5']\n",
    "\n",
    "path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd/'\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "list_input_anoms = []\n",
    "\n",
    "var_names = []\n",
    "for name_var in names_vars:\n",
    "    list_input_anoms.append(xr.open_dataset(f'{path_weekly_anoms}{name_var}.nc'))\n",
    "    var_names.append(list(list_input_anoms[-1].data_vars.keys())[0])\n",
    "    \n",
    "full_input_array = np.zeros((list_input_anoms[0][var_names[0]].values.shape[0],240,720,len(list_input_anoms)))\n",
    "\n",
    "for ichannel in range(len(list_input_anoms)):\n",
    "    full_input_array[:,:,:,ichannel] = list_input_anoms[ichannel][var_names[ichannel]].values\n",
    "    \n",
    "#     aaaaaa\n",
    "full_input_array[np.isfinite(full_input_array)==False]=0\n",
    "\n",
    "del(list_input_anoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab614fde-c2e2-49e6-b181-fc1c31b7d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "## array_mean\n",
    "names_vars = ['Z500_ERA5','OLR_ERA5','U200_ERA5','U10_ERA5']\n",
    "\n",
    "path_weekly_mean = '/glade/scratch/jhayron/Data4Predictability/WeeklyDatasets/'\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "list_input_mean = []\n",
    "\n",
    "var_names = []\n",
    "for name_var in names_vars:\n",
    "    list_input_mean.append(xr.open_dataset(f'{path_weekly_mean}{name_var}.nc'))\n",
    "    var_names.append(list(list_input_mean[-1].data_vars.keys())[0])\n",
    "    \n",
    "full_input_array_mean = np.zeros((list_input_mean[0][var_names[0]].values.shape[0],240,720,len(list_input_mean)))\n",
    "\n",
    "for ichannel in range(len(list_input_mean)):\n",
    "    full_input_array_mean[:,:,:,ichannel] = list_input_mean[ichannel][var_names[ichannel]].values\n",
    "    \n",
    "#     aaaaaa\n",
    "full_input_array_mean[np.isfinite(full_input_array_mean)==False]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150b36f3-47ac-411a-8798-57ae0b846f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_array = np.concatenate((full_input_array, full_input_array_mean), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f4595a-dd26-4772-a707-1e68983df3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(full_input_array)\n",
    "del(full_input_array_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7d2342-bfb3-4673-a55b-94048052e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "da_input = xr.DataArray(\n",
    "    data=merged_array,\n",
    "    dims=[\"time\",\"lat\", \"lon\",\"channel\"],\n",
    "    coords=dict(\n",
    "        time=([\"time\"], list_input_mean[0][var_names[0]].time.values),\n",
    "        lat=([\"lat\"], list_input_mean[0][var_names[0]].lat.values),\n",
    "        lon=([\"lon\"], list_input_mean[0][var_names[0]].lon.values),\n",
    "        channel=([\"channel\"], np.arange(merged_array.shape[-1])),\n",
    "    )\n",
    ")\n",
    "\n",
    "reduction_factor = 3\n",
    "coarsened_data = da_input.coarsen(lat=reduction_factor, lon=reduction_factor).mean()\n",
    "# del(full_input_array)\n",
    "del(list_input_mean)\n",
    "del(var_names)\n",
    "del(da_input)\n",
    "\n",
    "# Example usage:\n",
    "start_year = 1981\n",
    "end_year = 2015\n",
    "normalized_data = normalize_data_with_scaling(coarsened_data, start_year, end_year)\n",
    "\n",
    "week_out=3\n",
    "week_out_str = f'week{week_out}'\n",
    "\n",
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)\n",
    "        \n",
    "# train_joint_dataset, val_joint_dataset, test_joint_dataset = \\\n",
    "#     create_datasets_multichannel(normalized_data, df_shifts, week_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c00a49ba-ff70-41f8-a788-2f19559411d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_netcdf('/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/atm_1p5deg.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dccbd-6fd0-4e92-84c5-28299377891c",
   "metadata": {},
   "source": [
    "# Ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af129220-01a2-4468-bb1b-1fcd5a88469c",
   "metadata": {},
   "source": [
    "IC_SODA.nc OHC100_SODA.nc OHC50_SODA.nc SD_ERA5.nc SST_SODA.nc STL_7cm_ERA5.nc SWVL_28cm_ERA5.nc U10_ERA5.nc IT_SODA.nc OHC200_SODA.nc OHC700_SODA.nc SSH_SODA.nc STL_1m_ERA5.nc STL_full_ERA5.nc SWVL_7cm_ERA5.nc U200_ERA5.nc MLD_SODA.nc OHC300_SODA.nc OLR_ERA5.nc SST_OISSTv2.nc STL_28cm_ERA5.nc SWVL_1m_ERA5.nc SWVL_full_ERA5.nc Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58295d0-2123-41f6-85a9-81ae86b2dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## array_anoms\n",
    "names_vars = ['IT_SODA','OHC100_SODA','SST_SODA','MLD_SODA']\n",
    "\n",
    "path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd/'\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "list_input_anoms = []\n",
    "\n",
    "var_names = []\n",
    "for name_var in names_vars:\n",
    "    list_input_anoms.append(xr.open_dataset(f'{path_weekly_anoms}{name_var}.nc'))\n",
    "    var_names.append(list(list_input_anoms[-1].data_vars.keys())[0])\n",
    "    \n",
    "full_input_array = np.zeros((list_input_anoms[0][var_names[0]].values.shape[0],240,720,len(list_input_anoms)))\n",
    "\n",
    "for ichannel in range(len(list_input_anoms)):\n",
    "    full_input_array[:,:,:,ichannel] = list_input_anoms[ichannel][var_names[ichannel]].values\n",
    "    \n",
    "#     aaaaaa\n",
    "full_input_array[np.isfinite(full_input_array)==False]=0\n",
    "\n",
    "del(list_input_anoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8811f3-0b08-4385-897a-5efb3ecdce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## array_mean\n",
    "names_vars = ['IT_SODA','OHC100_SODA','SST_SODA','MLD_SODA']\n",
    "\n",
    "path_weekly_mean = '/glade/scratch/jhayron/Data4Predictability/WeeklyDatasets/'\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "list_input_mean = []\n",
    "\n",
    "var_names = []\n",
    "for name_var in names_vars:\n",
    "    list_input_mean.append(xr.open_dataset(f'{path_weekly_mean}{name_var}.nc'))\n",
    "    var_names.append(list(list_input_mean[-1].data_vars.keys())[0])\n",
    "    \n",
    "full_input_array_mean = np.zeros((list_input_mean[0][var_names[0]].values.shape[0],240,720,len(list_input_mean)))\n",
    "\n",
    "for ichannel in range(len(list_input_mean)):\n",
    "    full_input_array_mean[:,:,:,ichannel] = list_input_mean[ichannel][var_names[ichannel]].values\n",
    "    \n",
    "#     aaaaaa\n",
    "full_input_array_mean[np.isfinite(full_input_array_mean)==False]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "823705b1-9fae-462a-851c-7470aa25086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_array = np.concatenate((full_input_array, full_input_array_mean), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665087a5-f5ec-46f0-8e1c-79544d07f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(full_input_array)\n",
    "del(full_input_array_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60c630ee-ae8d-43a7-a32a-32702c970f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "da_input = xr.DataArray(\n",
    "    data=merged_array,\n",
    "    dims=[\"time\",\"lat\", \"lon\",\"channel\"],\n",
    "    coords=dict(\n",
    "        time=([\"time\"], list_input_mean[0][var_names[0]].time.values),\n",
    "        lat=([\"lat\"], list_input_mean[0][var_names[0]].lat.values),\n",
    "        lon=([\"lon\"], list_input_mean[0][var_names[0]].lon.values),\n",
    "        channel=([\"channel\"], np.arange(merged_array.shape[-1])),\n",
    "    )\n",
    ")\n",
    "\n",
    "reduction_factor = 3\n",
    "coarsened_data = da_input.coarsen(lat=reduction_factor, lon=reduction_factor).mean()\n",
    "# del(full_input_array)\n",
    "del(list_input_mean)\n",
    "del(var_names)\n",
    "del(da_input)\n",
    "\n",
    "# Example usage:\n",
    "start_year = 1981\n",
    "end_year = 2015\n",
    "normalized_data = normalize_data_with_scaling(coarsened_data, start_year, end_year)\n",
    "\n",
    "week_out=3\n",
    "week_out_str = f'week{week_out}'\n",
    "\n",
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)\n",
    "        \n",
    "# train_joint_dataset, val_joint_dataset, test_joint_dataset = \\\n",
    "#     create_datasets_multichannel(normalized_data, df_shifts, week_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634ec025-3d28-45de-96f7-167d36e79381",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_netcdf('/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/ocn_1p5deg.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad31a9-87f3-42df-abef-9231ed19e48c",
   "metadata": {},
   "source": [
    "# Land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c190f-f727-4884-a82d-4662c5a9badd",
   "metadata": {},
   "source": [
    "IC_SODA.nc OHC100_SODA.nc OHC50_SODA.nc SD_ERA5.nc SST_SODA.nc STL_7cm_ERA5.nc SWVL_28cm_ERA5.nc U10_ERA5.nc IT_SODA.nc OHC200_SODA.nc OHC700_SODA.nc SSH_SODA.nc STL_1m_ERA5.nc STL_full_ERA5.nc SWVL_7cm_ERA5.nc U200_ERA5.nc MLD_SODA.nc OHC300_SODA.nc OLR_ERA5.nc SST_OISSTv2.nc STL_28cm_ERA5.nc SWVL_1m_ERA5.nc SWVL_full_ERA5.nc Z500_ERA5.ncm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed36b11b-e813-4c6c-a601-346406818a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## array_anoms\n",
    "names_vars = ['SWVL_28cm_ERA5','SWVL_full_ERA5','STL_full_ERA5','SD_ERA5']\n",
    "\n",
    "path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_DetrendedStd/'\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "list_input_anoms = []\n",
    "\n",
    "var_names = []\n",
    "for name_var in names_vars:\n",
    "    list_input_anoms.append(xr.open_dataset(f'{path_weekly_anoms}{name_var}.nc'))\n",
    "    var_names.append(list(list_input_anoms[-1].data_vars.keys())[0])\n",
    "    \n",
    "full_input_array = np.zeros((list_input_anoms[0][var_names[0]].values.shape[0],240,720,len(list_input_anoms)))\n",
    "\n",
    "for ichannel in range(len(list_input_anoms)):\n",
    "    full_input_array[:,:,:,ichannel] = list_input_anoms[ichannel][var_names[ichannel]].values\n",
    "    \n",
    "#     aaaaaa\n",
    "full_input_array[np.isfinite(full_input_array)==False]=0\n",
    "\n",
    "del(list_input_anoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ef82c65-4250-4250-bb0e-0468b2db3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## array_mean\n",
    "names_vars = ['SWVL_28cm_ERA5','SWVL_full_ERA5','STL_full_ERA5','SD_ERA5']\n",
    "\n",
    "path_weekly_mean = '/glade/scratch/jhayron/Data4Predictability/WeeklyDatasets/'\n",
    "# path_weekly_anoms = '/glade/scratch/jhayron/Data4Predictability/WeeklyAnoms_Std_withTrends/'\n",
    "\n",
    "list_input_mean = []\n",
    "\n",
    "var_names = []\n",
    "for name_var in names_vars:\n",
    "    list_input_mean.append(xr.open_dataset(f'{path_weekly_mean}{name_var}.nc'))\n",
    "    var_names.append(list(list_input_mean[-1].data_vars.keys())[0])\n",
    "    \n",
    "full_input_array_mean = np.zeros((list_input_mean[0][var_names[0]].values.shape[0],240,720,len(list_input_mean)))\n",
    "\n",
    "for ichannel in range(len(list_input_mean)):\n",
    "    full_input_array_mean[:,:,:,ichannel] = list_input_mean[ichannel][var_names[ichannel]].values\n",
    "    \n",
    "#     aaaaaa\n",
    "full_input_array_mean[np.isfinite(full_input_array_mean)==False]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c2d4cf-b666-4ca1-befc-7eddb7c3228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_array = np.concatenate((full_input_array, full_input_array_mean), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "579d940d-23f0-4663-a46e-07cfcb620857",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(full_input_array)\n",
    "del(full_input_array_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4761bc60-7d8c-4b40-906a-34158966cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "da_input = xr.DataArray(\n",
    "    data=merged_array,\n",
    "    dims=[\"time\",\"lat\", \"lon\",\"channel\"],\n",
    "    coords=dict(\n",
    "        time=([\"time\"], list_input_mean[0][var_names[0]].time.values),\n",
    "        lat=([\"lat\"], list_input_mean[0][var_names[0]].lat.values),\n",
    "        lon=([\"lon\"], list_input_mean[0][var_names[0]].lon.values),\n",
    "        channel=([\"channel\"], np.arange(merged_array.shape[-1])),\n",
    "    )\n",
    ")\n",
    "\n",
    "reduction_factor = 3\n",
    "coarsened_data = da_input.coarsen(lat=reduction_factor, lon=reduction_factor).mean()\n",
    "# del(full_input_array)\n",
    "del(list_input_mean)\n",
    "del(var_names)\n",
    "del(da_input)\n",
    "\n",
    "# Example usage:\n",
    "start_year = 1981\n",
    "end_year = 2015\n",
    "normalized_data = normalize_data_with_scaling(coarsened_data, start_year, end_year)\n",
    "\n",
    "week_out=3\n",
    "week_out_str = f'week{week_out}'\n",
    "\n",
    "wr_series = pd.read_csv('/glade/work/jhayron/Data4Predictability/WR_Series_20230824.csv',\\\n",
    "                index_col=0,names=['week0'],skiprows=1,parse_dates=True)\n",
    "for wk in range(2,10):\n",
    "    series_temp = copy.deepcopy(wr_series[\"week0\"])\n",
    "    series_temp.index = series_temp.index - timedelta(weeks = wk-1)\n",
    "    series_temp.name = f'week{wk-1}'\n",
    "    if wk==2:\n",
    "        df_shifts = pd.concat([pd.DataFrame(wr_series[\"week0\"]),pd.DataFrame(series_temp)],axis=1)  \n",
    "    else:\n",
    "        df_shifts = pd.concat([df_shifts,pd.DataFrame(series_temp)],axis=1)\n",
    "        \n",
    "# train_joint_dataset, val_joint_dataset, test_joint_dataset = \\\n",
    "#     create_datasets_multichannel(normalized_data, df_shifts, week_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbb89acb-08c0-4820-9f65-82c09cff09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.to_netcdf('/glade/scratch/jhayron/Data4Predictability/FinalTrainingDataArrays/lnd_1p5deg.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc027e-745c-47b8-afd8-dd0ddc905807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_wr]",
   "language": "python",
   "name": "conda-env-cnn_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
